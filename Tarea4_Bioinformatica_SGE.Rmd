---
title: "Tarea 4"
output: html_notebook
---

---
title: "Estimador de lambda en datos Exonenciales con a priori Gamma"
author: "Sofia Gamino Estrada"
date: "October 19, 2024"
output: html_document
---
---

En este código se puede dar una densidad de posibles valores para un parámetro lambda desconocido de una distribución Exponencial en un intervalo de confianza por medio de inferencia Bayesiana

**Input**:
- lambda: Para poder comparar el desempeño del modelo y definir los datos de prueba 
- alpha: Parametro de forma a priori de Gamma
- beta: Parámetro de tasa a priori de Gamma
- p_range: Una secuencia de posibles valores para lambda. Se va a operar sobre esta
**Output**: 
- Distribución a priori
- Distribución a posteriori
- Estimadores a posteriori: MAP, Media, Mediana
- Intervalo de credibilidad de lambda del 95%: Son los posibles valores de lambda
- Distribución de Verosimilitud, así como MV y intervalo de confianza: Para comparación con el método Bayesiano

---

# Descripción del problema
 Sea $X$ una variable aleatoria Exponencial con parámetro de tasa $\lambda$ y con función de densidad:
$$ f_{X|\Lambda} (x|\lambda) = \begin{cases}
\lambda e^{-\lambda x }, & x>0;\\
0, & \text{otro caso}.
\end{cases}
$$
Encuentre la función de densidad a posteriori de $\Lambda$ a partir de una muestra de tamaño $n$, bajo el supuesto de que $\Lambda$ sigue una distribución a priori Gamma$(\alpha, \beta)$, donde $\alpha$ es parámetro de forma y $\beta$ es parámetro de tasa.

# Distribución Conjugada

## Hallar los kernel

Primero tenemos que hallar el kernel de $f_{X|\Lambda} (x|\lambda)$ y de $f_{\Lambda}(\lambda)$

### Kernel de $f_{X|\Lambda} (x|\lambda)$

$$f_{X|\Lambda} (x|\lambda) = 
\lambda e^{-\lambda x }
$$

$$L(\lambda) =
\prod^n_{i=1}\lambda e^{-\lambda x }
$$

$$L(\lambda)=
\lambda^{n}e^{-\lambda s}, s=\sum^n_{i=1}x_i
$$

### Kernel de $f_{\Lambda}(\lambda)$

$$f_{\Lambda}(\lambda) = \frac{\beta ^ \alpha}{\Gamma(\alpha)} \lambda^{\alpha-1} e^{-\beta\lambda}
$$
$$f_{\Lambda}(\lambda) \propto \lambda^{\alpha-1}e^{-\beta\lambda}
$$

## Distribución Conjugada a posteriori
Ya que tenemos los kernel, podemos sacar la distribución conjugada a posteriori. En este caso, ignoraremos la constante de normalización $\int¹_{0}L(\lambda)f_{\Lambda}(\lambda)d\lambda$ en el denominador.

$$(\lambda|x_1, ..., x_n) = L(\lambda)f_{\Lambda}(\lambda)
$$

$$(\lambda|x_1, ..., x_n) = 
\lambda^{\alpha-1}e^{-\beta\lambda}\lambda^{n}e^{-\lambda s}
$$

$$(\lambda|x_1, ..., x_n) = 
\lambda^{\alpha-1+n}e^{-\lambda s-\beta\lambda}
$$

$$(\lambda|x_1, ..., x_n) = 
\lambda^{\alpha^*-1}e^{-\lambda\beta^*}, \alpha^*=\alpha+n,\space b^*=s+\beta
$$

## Estimación de MAP

Podemos hallar MAP de lambda a partir de encontrar cuando la distribución conjugada es 0. Se hace ln(L($\lambda$)), después se deriva $\lambda$ y finalmente igualamos a 0.

$$l(\lambda) = ln[\lambda^{\alpha^*-1}e^{-\lambda\beta^*}]
$$

$$l(\lambda) = ln(\lambda^{\alpha^{*}-1})+ln(e^{-\lambda\beta^*})
$$

$$l(\lambda) = (\alpha^{*}-1)ln(\lambda)-\lambda\beta^*
$$

$$\frac{\partial l(\lambda)}{\partial \lambda} = \frac{\partial}{\partial\lambda}[(\alpha^{*}-1)ln(\lambda)-\lambda\beta^*]
$$

$$ = \frac{\alpha^{*}-1}{\lambda}-\beta^{*}
$$

$$ \frac{\alpha^{*}-1}{\lambda}-\beta^{*} = 0
$$ 

$$\alpha^{*}-1 = \beta^{*}\lambda
$$

$$\lambda^{-}_{MAP} =\frac{\alpha^{*}-1}{\beta^{*}}
$$

# Código de estimador bayesiano

## Creación de datos de prueba
Esto nos va a poder ayudar a comparar nuestro estimador bayesiano con el valor real de $\lambda$ a partir de una distribución Exponencial.

```{r}
rm(list = ls())
lambda = 4
n = 30
set.seed(1010)
x = rexp(n, lambda)

```


## A priori Gamma no informativa de lambda

```{r}
alpha =.0000001
beta =.0000001
p_range = seq(0, 10, .01)
priori_l = dgamma(p_range, rate = beta, shape = alpha)
plot(p_range, priori_l, col="magenta", type ="l", lwd = 2, xlab = "lambda", ylab = "f(lambda)", main = "Densidad a priori Gamma para lambda")
```

## A posteriori Gamma de lambda

```{r}
alpha_p = alpha + n
beta_p = sum(x) + beta

post_l = dgamma(p_range, rate = beta_p, shape = alpha_p)

plot(p_range, post_l, col="dodgerblue", type ="l", lwd = 2, xlab = "lambda", ylab = "f(lambda)", main = "Densidad a Posteriori vs a Priori")
lines(p_range, priori_l, col="magenta", type ="l", lwd = 2)

IC_cred = qgamma(c(0.025, 0.975), shape = alpha_p, rate = beta_p)

MAP = (alpha_p-1)/beta_p
Mean = alpha_p/beta_p
Median = qgamma(0.5, shape = alpha_p, rate = beta_p)

abline(v = MAP, lwd = 2, col = "darkmagenta", lty = 4)
abline(v = Mean, lwd = 2, col = "blue4", lty = 4)
abline(v = Median, lwd = 2, col = "green", lty = 4)
abline(v = IC_cred[1], lwd = 2, col = "red", lty = 5)
abline(v = IC_cred[2], lwd = 2, col = "red", lty = 6)
abline(v = lambda, lwd = 2, col = "black", lty = 1)
legend("topright", c("MAP", "Mean", "Median", "IC_cred_lower", "IC_cred_upper", "Real value"), lty = c(4, 4, 4, 5, 6, 1), col = c("darkmagenta", "blue4", "green", "red", "red", "black"))


cat("MAP = ", MAP, "\n")
cat("Mean = ", Mean, "\n")
cat("Median = ", Median, "\n")
cat("Intervalo de credibilidad = ", IC_cred, "\n")
cat("Valor Real = ", lambda, "\n")

```

## Función de Verosimilitud

```{r}

verosim_v = c()
for (l in p_range) {
  verosim = (l^{n})*(exp(-l*sum(x)))
  verosim_v = append(verosim_v, verosim)
}

MV = 1/mean(x)

threshold = MV * exp(-qchisq(0.95, df = 1) / 2)

IC_lower = min(p_range[verosim_v >= threshold])
IC_upper = max(p_range[verosim_v >= threshold])

plot(p_range, verosim_v, col="purple", type ="l", lwd = 2, xlab = "lambda", ylab = "L(lambda)", main = "Gráfica de verosimilitud")
abline(v = MV, lwd = 2, col = "gold", lty = 4)
abline(v = IC_lower , lwd = 2, col = "blue4", lty = 4)
abline(v = IC_upper, lwd = 2, col = "blue4", lty = 4)
abline(v = lambda, lwd = 2, col = "black", lty = 1)
legend("topright", c("MV", "Int_conf", "Valor Real"), lty = c(4, 4, 1), col = c("gold", "blue4", "black"))


cat("MV = ", MV, "\n")
cat("Confidence Interval = ", IC_lower, ",", IC_upper)
```

---
title: "Estimador de tau en datos Normales con a priori Gamma"
author: "Sofia Gamino Estrada"
date: "October 19, 2024"
output: html_document
---
---

En este código se puede dar una densidad de posibles valores para un parámetro tau de precision desconocido de una distribución Normal en un intervalo de confianza a partir de un tamaño de muestra n conocido y media mu conocida por medio de inferencia Bayesiana

**Input**:
- tau: Para poder comparar el desempeño del modelo y definir los datos de prueba 
- alpha: Parametro de forma a priori de Gamma
- beta: Parámetro de tasa a priori de Gamma
- p_range: Una secuencia de posibles valores para lambda. Se va a operar sobre esta
- n: El tamaño de la muestra 
- mu: La media de la distribución 
**Output**: 
- Distribución a priori
- Distribución a posteriori
- Estimadores a posteriori: MAP, Media, Mediana
- Intervalo de credibilidad de lambda del 95%: Son los posibles valores de lambda
- Distribución de Verosimilitud, así como MV y intervalo de confianza: Para comparación con el método Bayesiano

---

# Descripción del problema
Sea $X$ una variable aleatoria Normal con media conocida $\mu$ y precisión $\tau$ desconocida, cuya función de densidad está dada por
$$ f_{X|T}(x|\tau) = \sqrt{\frac{\tau}{2\pi}}\exp\left\{-\frac{\tau}{2} (x-\mu)^2 \right\}.
$$
Note que la precisión es igual a $\sigma^{-2}$, lo cual implica que, a mayor varianza, menor precisión, y a mayor precisión, menor varianza, puesto que uno es el inverso del otro. Encuentre la función de densidad a posteriori de $T$ a partir de una muestra de tamaño $n$, bajo el supuesto de que $T$ sigue una distribución a priori Gamma$(\alpha, \beta)$, donde $\alpha$ es parámetro de forma y $\beta$ es parámetro de tasa.

# Distribución Conjugada

## Hallar los kernel

Primero tenemos que hallar el kernel de $f_{X|T} (x|\tau)$ y de $f_{T}(\tau)$

### Kernel de $f_{X|T} (x|\tau)$

$$f_{X|T} (x|\tau) = 
\sqrt{\frac{\tau}{2\pi}}\exp\left\{-\frac{\tau}{2} (x-\mu)^2 \right\}
$$
$$f_{X|\Lambda} (x|\lambda) =
(\frac{\tau}{2\pi})^{\frac{1}{2}}\exp\left\{-\frac{\tau}{2} (x-\mu)^2 \right\}
$$

$$L(\tau) = \prod^n_{i=1}(f_{X|\Lambda} (x|\lambda)) = 
\prod^{n}_{i=1}(\frac{\tau}{2\pi})^{\frac{1}{2}}\exp\left\{-\frac{\tau}{2} (x-\mu)^2 \right\}
$$

$$L(\tau) = 
(\frac{\tau}{2\pi})^{\frac{n}{2}}\exp\left\{-\frac{1}{2}\tau \sum^n_{i=1}(x_i-\mu)^2 \right\}
$$

$$L(\tau) =
\tau^{\frac{n}{2}}\exp\left\{-\frac{\tau }{2}\sum^n_{i=1}(x_i-\mu)^2 \right\}
$$


### Kernel de $f_{T}(\tau)$

$$f_{T}(\tau) = \frac{\beta ^ \alpha}{\Gamma(\alpha)} \tau^{\alpha-1} e^{-\beta\}
$$
$$f_{T}(\tau) \propto \tau^{\alpha-1}e^{-\beta\tau}
$$

## Distribución Conjugada a posteriori
Ya que tenemos los kernel, podemos sacar la distribución conjugada a posteriori. En este caso, ignoraremos la constante de normalización $\int¹_{0}L(\tau)f_{T}(\tau)d\tau$ en el denominador.

$$(\tau|x_1, ..., x_n) = L(\tau)f_{T}(\tau)
$$

$$(\tau|x_1, ..., x_n) = 
\tau^{\alpha-1}e^{-\beta\tau}\tau^{\frac{n}{2}}\exp\left\{-\frac{\tau }{2}\sum^n_{i=1}(x_i-\mu)^2 \right\}
$$

$$(\tau|x_1, ..., x_n) = \tau^{\alpha-1+\frac{n}{2}}\exp\left\{-\frac{\tau }{2}\sum^n_{i=1}(x_i-\mu)^2-\beta\tau \right\}
$$

$$(\tau|x_1, ..., x_n) = \tau^{\alpha-1+\frac{1}{2}}\exp\left\{-\tau(\frac{1}{2}\sum^n_{i=1}(x_i-\mu)^2-\beta) \right\}
$$

$$(\tau|x_1, ..., x_n) = 
\tau^{\alpha^*-1}e^{-\tau\beta^*}, \alpha^*=\alpha+\frac{n}{2},\space b^*=\frac{1}{2}\sum^n_{i=1}(x_i-\mu)^2-\beta
$$

## Estimación de MAP

Podemos hallar MAP de lambda a partir de encontrar cuando la distribución conjugada es 0. Se hace ln(L($\tau$)), después se deriva $\lambda$ y finalmente igualamos a 0.

$$l(\tau) = ln[\tau^{\alpha^*-1}e^{-\tau\beta^*}]
$$

$$l(\tau) = ln(\tau^{\alpha^{*}-1})+ln(e^{-\tau\beta^*})
$$

$$l(\tau) = (\alpha^{*}-1)ln(\tau)-\tau\beta^*
$$

$$\frac{\partial l(\tau)}{\partial \tau} = \frac{\partial}{\partial\tau}[(\alpha^{*}-1)ln(\tau)-\tau\beta^*]
$$

$$ = \frac{\alpha^{*}-1}{\tau}-\beta^{*}
$$

$$ \frac{\alpha^{*}-1}{\tau}-\beta^{*} = 0
$$ 

$$\alpha^{*}-1 = \beta^{*}\tau
$$

$$\tau^{-}_{MAP} =\frac{\alpha^{*}-1}{\beta^{*}}
$$

# Código de estimador bayesiano

## Creación de datos de prueba
Esto nos va a poder ayudar a comparar nuestro estimador bayesiano con el valor real de $\tau$ a partir de una distribución Poisson.

```{r}
rm(list = ls())
tau = 7
mu = 5
n = 30
set.seed(1010)
x = rnorm(n, mean = mu, sd = 1/sqrt(tau))

```


## A priori Gamma no informativa de lambda

```{r}
alpha =1
beta =1
p_range = seq(0, 10, .01)
priori_l = dgamma(p_range, rate = beta, shape = alpha)
plot(p_range, priori_l, col="magenta", type ="l", lwd = 2, xlab = "tau", ylab = "f(tau)", main = "Densidad a priori Gamma para tau")
```

## A posteriori Gamma de lambda

```{r}

suma_cuadrados = sum((x - mu)^2)  # Suma de los cuadrados de las diferencias

alpha_p = alpha + (n/2)
beta_p = suma_cuadrados-beta

post_l = dgamma(p_range, rate = beta_p, shape = alpha_p)

plot(p_range, post_l, col="dodgerblue", type ="l", lwd = 2, xlab = "tau", ylab = "f(tau)", main = "Densidad a Posteriori vs a Priori")
lines(p_range, priori_l, col="magenta", type ="l", lwd = 2)

IC_cred = qgamma(c(0.025, 0.975), shape = alpha_p, rate = beta_p)

MAP = (alpha_p-1)/beta_p
Mean = alpha_p/beta_p
Median = qgamma(0.5, shape = alpha_p, rate = beta_p)

abline(v = MAP, lwd = 2, col = "darkmagenta", lty = 4)
abline(v = Mean, lwd = 2, col = "blue4", lty = 4)
abline(v = Median, lwd = 2, col = "green", lty = 4)
abline(v = IC_cred[1], lwd = 2, col = "red", lty = 5)
abline(v = IC_cred[2], lwd = 2, col = "red", lty = 6)
abline(v = tau, lwd = 2, col = "black", lty = 1)
legend("topright", c("MAP", "Mean", "Median", "IC_cred_lower", "IC_cred_upper", "Real value"), lty = c(4, 4, 4, 5, 6, 1), col = c("darkmagenta", "blue4", "green", "red", "red", "black"))


cat("MAP = ", MAP, "\n")
cat("Mean = ", Mean, "\n")
cat("Median = ", Median, "\n")
cat("Intervalo de credibilidad = ", IC_cred, "\n")
cat("Valor Real = ", tau, "\n")

```

## Función de Verosimilitud

```{r}

verosim_v = c()
suma_cuadrados = sum((x - mu)^2)  # Suma de los cuadrados de las diferencias

for (t in p_range) {
  verosim = (t / (2 * pi))^(n / 2) * exp(-t * suma_cuadrados / 2)
  verosim_v = append(verosim_v, verosim)
}

MV = n/suma_cuadrados

threshold = MV * exp(-qchisq(0.95, df = 1) / 2)

IC_lower = min(p_range[verosim_v >= threshold])
IC_upper = max(p_range[verosim_v >= threshold])

plot(p_range, verosim_v, col="purple", type ="l", lwd = 2, xlab = "lambda", ylab = "L(lambda)", main = "Gráfica de verosimilitud")
abline(v = MV, lwd = 2, col = "gold", lty = 4)
abline(v = IC_lower , lwd = 2, col = "blue4", lty = 4)
abline(v = IC_upper, lwd = 2, col = "blue4", lty = 4)
abline(v = tau, lwd = 2, col = "black", lty = 1)
legend("topright", c("MV", "Int_conf", "Valor Real"), lty = c(4, 4, 1), col = c("gold", "blue4", "black"))


cat("MV = ", MV, "\n")
cat("Confidence Interval = ", IC_lower, ",", IC_upper)
```

---
title: "Estimador de lambda en datos Poisson con a priori Gamma"
author: "Sofia Gamino Estrada"
date: "October 19, 2024"
output: html_document
---
---

En este código se puede dar una densidad de posibles valores para un parámetro lambda desconocido de una distribución Poisson en un intervalo de confianza a partir de un tamaño de muestra n conocido por medio de inferencia Bayesiana

**Input**:
- lambda: Para poder comparar el desempeño del modelo y definir los datos de prueba 
- alpha: Parametro de forma a priori de Gamma
- beta: Parámetro de tasa a priori de Gamma
- p_range: Una secuencia de posibles valores para lambda. Se va a operar sobre esta
- n: El tamaño de la muestra 
**Output**: 
- Distribución a priori
- Distribución a posteriori
- Estimadores a posteriori: MAP, Media, Mediana
- Intervalo de credibilidad de lambda del 95%: Son los posibles valores de lambda
- Distribución de Verosimilitud, así como MV y intervalo de confianza: Para comparación con el método Bayesiano

---

# Descripción del problema
Se desea estimar el parámetro $\lambda$ de una distribución Poisson a partir de una muestra de tamaño $n$. Encuentre la distribución posterior de $\Lambda$ suponiendo que, a priori, $\Lambda\sim \text{Gamma}(\alpha, \beta)$, donde $\alpha$ es parámetro de forma y $\beta$ es parámetro de tasa. Recuerde la función de probabilidad Poisson está dada por:
$$ f_{X|\Lambda}(x|\lambda) = \begin{cases}
\frac{\lambda^x e^{-\lambda}}{x!}, & x=0,1,2,...;\\
0 & \text{otro caso}.
\end{cases}
$$

# Distribución Conjugada

## Hallar los kernel

Primero tenemos que hallar el kernel de $f_{X|\Lambda} (x|\lambda)$ y de $f_{\Lambda}(\lambda)$

### Kernel de $f_{X|\Lambda} (x|\lambda)$

$$f_{X|\Lambda} (x|\lambda) = 
\frac{\lambda^x e^{-\lambda}}{x!}
$$
$$f_{X|\Lambda} (x|\lambda) \propto \lambda^{x}e^{-\lambda}
$$
$$L(\lambda) = \prod^n_{i=1}(f_{X|\Lambda} (x|\lambda)) = \prod^n_{i=1}\lambda^{x}e^{-\lambda}
$$
$$L(\lambda) = \lambda^{s}e^{-\lambda n}, s = \sum^n_{i=1}x_i
$$

### Kernel de $f_{\Lambda}(\lambda)$

$$f_{\Lambda}(\lambda) = \frac{\beta ^ \alpha}{\Gamma(\alpha)} \lambda^{\alpha-1} e^{-\beta\lambda}
$$
$$f_{\Lambda}(\lambda) \propto \lambda^{\alpha-1}e^{-\beta\lambda}
$$

## Distribución Conjugada a posteriori
Ya que tenemos los kernel, podemos sacar la distribución conjugada a posteriori. En este caso, ignoraremos la constante de normalización $\int¹_{0}L(\lambda)f_{\Lambda}(\lambda)d\lambda$ en el denominador.

$$(\lambda|x_1, ..., x_n) = L(\lambda)f_{\Lambda}(\lambda)
$$

$$(\lambda|x_1, ..., x_n) = 
\lambda^{s}e^{-\lambda n}\lambda^{\alpha-1}e^{-\beta\lambda}
$$

$$(\lambda|x_1, ..., x_n) = \lambda^{s+\alpha-1}e^{-\lambda n-\beta\lambda}
$$

$$(\lambda|x_1, ..., x_n) = 
\lambda^{\alpha^*-1}e^{-\lambda\beta^*}, \alpha^*=s+\alpha,\space b^*=n+\beta
$$

## Estimación de MAP

Podemos hallar MAP de lambda a partir de encontrar cuando la distribución conjugada es 0. Se hace ln(L($\lambda$)), después se deriva $\lambda$ y finalmente igualamos a 0.

$$l(\lambda) = ln[\lambda^{\alpha^*-1}e^{-\lambda\beta^*}]
$$

$$l(\lambda) = ln(\lambda^{\alpha^{*}-1})+ln(e^{-\lambda\beta^*})
$$

$$l(\lambda) = (\alpha^{*}-1)ln(\lambda)-\lambda\beta^*
$$

$$\frac{\partial l(\lambda)}{\partial \lambda} = \frac{\partial}{\partial\lambda}[(\alpha^{*}-1)ln(\lambda)-\lambda\beta^*]
$$

$$ = \frac{\alpha^{*}-1}{\lambda}-\beta^{*}
$$

$$ \frac{\alpha^{*}-1}{\lambda}-\beta^{*} = 0
$$ 

$$\alpha^{*}-1 = \beta^{*}\lambda
$$

$$\lambda^{-}_{MAP} =\frac{\alpha^{*}-1}{\beta^{*}}
$$

# Código de estimador bayesiano

## Creación de datos de prueba
Esto nos va a poder ayudar a comparar nuestro estimador bayesiano con el valor real de $\lambda$ a partir de una distribución Poisson.

```{r}
rm(list = ls())
lambda = 7
n = 30
set.seed(1010)
x = rpois(n, lambda)

```


## A priori Gamma no informativa de lambda

```{r}
alpha =.0000001
beta =.0000001
p_range = seq(0, 10, .01)
priori_l = dgamma(p_range, rate = beta, shape = alpha)
plot(p_range, priori_l, col="magenta", type ="l", lwd = 2, xlab = "lambda", ylab = "f(lambda)", main = "Densidad a priori Gamma para lambda")
```

## A posteriori Gamma de lambda

```{r}
alpha_p = sum(x) + alpha
beta_p = n + beta

post_l = dgamma(p_range, rate = beta_p, shape = alpha_p)

plot(p_range, post_l, col="dodgerblue", type ="l", lwd = 2, xlab = "lambda", ylab = "f(lambda)", main = "Densidad a Posteriori vs a Priori")
lines(p_range, priori_l, col="magenta", type ="l", lwd = 2)

IC_cred = qgamma(c(0.025, 0.975), shape = alpha_p, rate = beta_p)

MAP = (alpha_p-1)/beta_p
Mean = alpha_p/beta_p
Median = qgamma(0.5, shape = alpha_p, rate = beta_p)

abline(v = MAP, lwd = 2, col = "darkmagenta", lty = 4)
abline(v = Mean, lwd = 2, col = "blue4", lty = 4)
abline(v = Median, lwd = 2, col = "green", lty = 4)
abline(v = IC_cred[1], lwd = 2, col = "red", lty = 5)
abline(v = IC_cred[2], lwd = 2, col = "red", lty = 6)
abline(v = lambda, lwd = 2, col = "black", lty = 1)
legend("topright", c("MAP", "Mean", "Median", "IC_cred_lower", "IC_cred_upper", "Real value"), lty = c(4, 4, 4, 5, 6, 1), col = c("darkmagenta", "blue4", "green", "red", "red", "black"))


cat("MAP = ", MAP, "\n")
cat("Mean = ", Mean, "\n")
cat("Median = ", Median, "\n")
cat("Intervalo de credibilidad = ", IC_cred, "\n")
cat("Valor Real = ", lambda, "\n")

```

## Función de Verosimilitud

```{r}

verosim_v = c()
for (l in p_range) {
  verosim = (l^{sum(x)})*(exp(-l*n))
  verosim_v = append(verosim_v, verosim)
}

MV = mean(x)

threshold = MV * exp(-qchisq(0.95, df = 1) / 2)

IC_lower = min(p_range[verosim_v >= threshold])
IC_upper = max(p_range[verosim_v >= threshold])

plot(p_range, verosim_v, col="purple", type ="l", lwd = 2, xlab = "lambda", ylab = "L(lambda)", main = "Gráfica de verosimilitud")
abline(v = MV, lwd = 2, col = "gold", lty = 4)
abline(v = IC_lower , lwd = 2, col = "blue4", lty = 4)
abline(v = IC_upper, lwd = 2, col = "blue4", lty = 4)
abline(v = lambda, lwd = 2, col = "black", lty = 1)
legend("topright", c("MV", "Int_conf", "Valor Real"), lty = c(4, 4, 1), col = c("gold", "blue4", "black"))


cat("MV = ", MV, "\n")
cat("Confidence Interval = ", IC_lower, ",", IC_upper)
```

---
title: "Estimador de lamda en datos Gamma con a priori Gamma"
author: "Sofia Gamino Estrada"
date: "October 19, 2024"
output: html_document
---
---

En este código se puede dar una densidad de posibles valores para un parámetro lambda desconocido de una distribución Gamma en un intervalo de confianza a partir de alpha conocido por medio de inferencia Bayesiana

**Input**:
- alpha: Es el parametro conocido de la función a la que pertenecen los datos
- lambda: Para poder comparar el desempeño del modelo y definir los datos de prueba 
- a: Parametro de forma a priori
- b: Parámetro de tasa a priori
- p_range: Una secuencia de posibles valores para lambda. Se va a operar sobre esta
- n: El tamaño de la muestra 
**Output**: 
- Distribución a priori
- Distribución a posteriori
- Estimadores a posteriori: MAP, Media, Mediana
- Intervalo de credibilidad de lambda del 95%: Son los posibles valores de lambda
- Distribución de Verosimilitud, así como MV y intervalo de confianza: Para comparación con el método Bayesiano

---

# Descripción del problema
Sea $X$ una variable aleatoria Gamma de parámetro de forma $\alpha$ conocido y parámetro de tasa $\lambda$ desconocido, con función de densidad
$$ f_{X|\Lambda} (x|\lambda) = \begin{cases}
\frac{ \lambda ^ \alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-x \lambda}, & x>0;\\
0, & \text{otro caso}.
\end{cases}
$$
Encuentre la distribución posterior de $\Lambda$ suponiendo que se tiene una muestra de tamaño $n$ y que, a priori, $\Lambda$ sigue una distribución Gamma$(a, b)$. Luego, la función de densidad a priori es:
$$ f_{\Lambda}(\lambda) = \begin{cases}
\frac{ b ^ a}{\Gamma(a)} \lambda^{a-1} e^{-b\lambda}, & x>0;\\
0, & \text{otro caso}.
\end{cases}
$$

# Distribución Conjugada

## Hallar los kernel

Primero tenemos que hallar el kernel de $f_{X|\Lambda} (x|\lambda)$ y de $f_{\Lambda}(\lambda)$

### Kernel de $f_{X|\Lambda} (x|\lambda)$

$$f_{X|\Lambda} (x|\lambda) = \frac{1}{\Gamma(\alpha)}\lambda^{\alpha}x^{\alpha-1}e^{-x\lambda}
$$
$$f_{X|\Lambda} (x|\lambda) \propto \lambda^{\alpha}e^{-x\lambda}
$$
$$L(\lambda) = \prod^n_{i=1}(f_{X|\Lambda} (x|\lambda)) = \prod^n_{i=1}\lambda^{\alpha}e^{-x\lambda}
$$
$$L(\lambda) = \lambda^{n\alpha}e^{-s\lambda}, s = \sum^n_{i=1}x_i
$$

### Kernel de $f_{\Lambda}(\lambda)$

$$f_{\Lambda}(\lambda) = \frac{ b ^ a}{\Gamma(a)} \lambda^{a-1} e^{-b\lambda}
$$
$$f_{\Lambda}(\lambda) \propto \lambda^{a-1}e^{-b\lambda}
$$

## Distribución Conjugada a posteriori
Ya que tenemos los kernel, podemos sacar la distribución conjugada a posteriori. En este caso, ignoraremos la constante de normalización $\int¹_{0}L(\lambda)f_{\Lambda}(\lambda)d\lambda$ en el denominador.

$$(\lambda|x_1, ..., x_n) = L(\lambda)f_{\Lambda}(\lambda)
$$

$$(\lambda|x_1, ..., x_n) = \lambda^{n\alpha}e^{-s\lambda}\lambda^{a-1}e^{-b\lambda}
$$

$$(\lambda|x_1, ..., x_n) = \lambda^{n\alpha+a-1}e^{-s\lambda-b\lambda}
$$

$$(\lambda|x_1, ..., x_n) = \lambda^{a^*-1}e^{-b^*\lambda}, a^*=n\alpha+a,\space b^*=s+b
$$

## Estimación de MAP

Podemos hallar MAP de lambda a partir de encontrar cuando la distribución conjugada es 0. Se hace ln(L($\lambda$)), después se deriva $\lambda$ y finalmente igualamos a 0.

$$l(\lambda) = ln[\lambda^{a^*-1}e^{-b^*\lambda}]
$$

$$l(\lambda) = ln(\lambda^{a^{*}-1})+ln(e^{-b^{*}\lambda})
$$

$$l(\lambda) = a^{*}-1ln(\lambda)-b^{*}\lambda
$$

$$\frac{\partial l(\lambda)}{\partial \lambda} = \frac{\partial}{\partial\lambda}[a^{*}-1ln(\lambda)-b^{*}\lambda]
$$

$$ = \frac{a^{*}-1}{\lambda}-b^{*}
$$

$$ \frac{a^{*}-1}{\lambda}-b^{*} = 0
$$ 

$$a^{*}-1 = b^{*}\lambda
$$

$$\lambda^{-}_{MAP} =\frac{a^{*}-1}{b^{*}}
$$

# Código de estimador bayesiano

## Creación de datos de prueba
Esto nos va a poder ayudar a comparar nuestro estimador bayesiano con el valor real de $\lambda$ a partir de una distribución Gamma.

```{r}
rm(list = ls())
alpha = 2
lambda = 5
n = 30
set.seed(1010)
x = rgamma(n, rate = lambda, shape = alpha)

```


## A priori Gamma no informativa de lambda

```{r}
a =10
b =1/10
p_range = seq(0, 15, .01)
priori_l = dgamma(p_range, rate = b, shape = a)
plot(p_range, priori_l, col="magenta", type ="l", lwd = 2, xlab = "lambda", ylab = "f(lambda)", main = "Densidad a priori Gamma para lambda")
```

## A posteriori Gamma de lambda

```{r}
a_p = n*alpha + a
b_p = sum(x) + b

post_l = dgamma(p_range, rate = b_p, shape = a_p)

plot(p_range, post_l, col="dodgerblue", type ="l", lwd = 2, xlab = "lambda", ylab = "f(lambda)", main = "Densidad a Posteriori vs a Priori")
lines(p_range, priori_l, col="magenta", type ="l", lwd = 2)

IC_cred = qgamma(c(0.025, 0.975), shape = a_p, rate = b_p)

MAP = (a_p-1)/b_p
Mean = a_p/b_p
Median = qgamma(0.5, shape = a_p, rate = b_p)

abline(v = MAP, lwd = 2, col = "darkmagenta", lty = 4)
abline(v = Mean, lwd = 2, col = "blue4", lty = 4)
abline(v = Median, lwd = 2, col = "green", lty = 4)
abline(v = IC_cred[1], lwd = 2, col = "red", lty = 5)
abline(v = IC_cred[2], lwd = 2, col = "red", lty = 6)
abline(v = lambda, lwd = 2, col = "black", lty = 1)
legend("topright", c("MAP", "Mean", "Median", "IC_cred_lower", "IC_cred_upper", "Real value"), lty = c(4, 4, 4, 5, 6, 1), col = c("darkmagenta", "blue4", "green", "red", "red", "black"))


cat("MAP = ", MAP, "\n")
cat("Mean = ", Mean, "\n")
cat("Median = ", Median, "\n")
cat("Intervalo de credibilidad = ", IC_cred, "\n")
cat("Valor Real = ", lambda, "\n")

```

## Función de Verosimilitud

```{r}

verosim_v = c()
for (l in p_range) {
  verosim = (l^{n*alpha})*(exp(-sum(x)*l))
  verosim_v = append(verosim_v, verosim)
}

MV = (alpha*n)/sum(x) 

threshold = MV * exp(-qchisq(0.95, df = 1) / 2)

IC_lower = min(p_range[verosim_v >= threshold])
IC_upper = max(p_range[verosim_v >= threshold])

plot(p_range, verosim_v, col="purple", type ="l", lwd = 2, xlab = "lambda", ylab = "L(lambda)", main = "Gráfica de verosimilitud")
abline(v = MV, lwd = 2, col = "gold", lty = 4)
abline(v = IC_lower , lwd = 2, col = "blue4", lty = 4)
abline(v = IC_upper, lwd = 2, col = "blue4", lty = 4)
abline(v = lambda, lwd = 2, col = "black", lty = 1)
legend("topright", c("MV", "Int_conf", "Valor Real"), lty = c(4, 4, 1), col = c("gold", "blue4", "black"))


cat("MV = ", MV, "\n")
cat("Confidence Interval = ", IC_lower, ",", IC_upper)
```

---
title: "Estimador de theta en datos de una Binomial Negativa con a priori Beta"
author: "Sofia Gamino Estrada"
date: "October 19, 2024"
output: html_document
---
---

En este código se puede dar una densidad de posibles valores para un parámetro theta desconocido de una Distribución Binomial Negativa en un intervalo de confianza a partir de r conocido por medio de inferencia Bayesiana

**Input**:
- n: Tamaño de la muestra
- theta: Para poder comparar el desempeño del modelo y definir los datos de prueba 
- r: La cantidad de exitos deseada
- alpha: Uno de los parámetros de la distribución beta
- beta: El otro parámetro de la distribución beta

**Output**: 
- Distribución a priori
- Distribución a posteriori
- Estimadores a posteriori: MAP, Media, Mediana
- Intervalo de credibilidad de lambda del 95%: Son los posibles valores de lambda
- Distribución de Verosimilitud, así como MV y intervalo de confianza(no sale): Para comparación con el método Bayesiano

---

# Descripción del problema
Sea $X$ una variable aleatoria Binomial Negativa de parámetros $r$ y $\theta$. Encontrar la distribución a posteriori de $\Theta$ suponiendo que se tiene una muestra de tamaño $n$ y que, a priori, $\Theta$ sigue una distribución Beta$(a,b)$. Recuerde que la varaible $X$ cuenta el número de fallas hasta obtener $r$ éxitos, por lo que la función de probabilidad está dada por:
$$ f_{X|\Theta}(x|\theta) = \begin{cases}
{x+r-1 \choose x} \theta^r(1-\theta)^x, & x=0,1,2,...;\\
0 & \text{otro caso}.
\end{cases}
$$

Distribución beta a priori
$$f_{\Theta}(\theta) = \begin{cases}
\frac{ 1}{B(\alpha,\beta)} x^{\alpha-1} (1-x)^{\beta-1}, & x>0;\\
0, & \text{otro caso}.
\end{cases}
$$

# Distribución Conjugada

## Hallar los kernel

Primero tenemos que hallar el kernel de $f_{X|\Theta} (x|\theta)$ y de $f_{\theta}(\theta)$

### Kernel de $f_{X|\Theta} (x|\theta)$

$$f_{X|\Theta} (x|\theta) = 
{x+r-1 \choose x} \theta^r(1-\theta)^x
$$

$$f_{X|\Theta} (x|\theta) \propto
\theta^r(1-\theta)^x
$$

$$L(\theta) = \prod^n_{i=1}(f_{X|\Theta} (x|\theta)) = \prod^n_{i=1}\theta^r(1-\theta)^x
$$

$$L(\theta) = \theta^{nr}(1-\theta)^{s}, s = \sum^n_{i=1}x_i
$$

### Kernel de $f_{\Lambda}(\lambda)$

$$f_{\Theta}(\theta) = 
\frac{ 1}{B(\alpha,\beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1}
$$

$$f_{\Theta}(\theta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}
$$

## Distribución Conjugada a posteriori
Ya que tenemos los kernel, podemos sacar la distribución conjugada a posteriori. En este caso, ignoraremos la constante de normalización $\int¹_{0}L(\theta)f_{\Theta}(\theta)d\theta$ en el denominador.

$$(\theta|x_1, ..., x_n) = 
L(\theta)f_{\Theta}(\theta)
$$

$$(\theta|x_1, ..., x_n) = 
\theta^{nr}(1-\theta)^{s}\theta^{\alpha-1} (1-\theta)^{\beta-1}
$$

$$(\theta|x_1, ..., x_n) = 
\theta^{nr+\alpha-1}(1-\theta)^{s+\beta-1}\ 
$$

$$(\theta|x_1, ..., x_n) = 
\theta^{a^*-1}(1-\theta)^{\beta^*-1}, \space \alpha^*=nr+\alpha, \space \beta^* = s+\beta
$$

## Estimación de MAP

Podemos hallar MAP de theta a partir de encontrar cuando la distribución conjugada es 0. Se hace ln(L($\theta$)), después se deriva $\theta$ y finalmente igualamos a 0.

$$l(\theta) = 
ln[\theta^{\alpha^*-1}(1-\theta)^{\beta^*-1}]
$$
$$l(\theta)=
ln(\theta^{\alpha^*-1})+ln((1-\theta)^{\beta^*-1})
$$

$$l(\theta) = 
(\alpha^*-1)ln(\theta)-(\beta^*-1)ln(1-\theta)
$$

$$\frac{\partial l(\theta)}{\partial \theta} = \frac{\partial}{\partial\theta}[(\alpha^*-1)ln(\theta)-(\beta^*-1)ln(1-\theta)]
$$

$$ = 
\frac{\alpha^*-1}{\theta}-\frac{\beta^*-1}{1-\theta}
$$

$$ = 
\frac{\alpha^*-1(1-\theta)-\theta(\beta^*-1)}{\theta(1-\theta)}
$$
$$\frac{\alpha^*-\alpha^*\theta-1+\theta-\beta^*\theta+\theta}{\theta(1-\theta)}=0
$$
$$-\alpha^*\theta-\beta^*\theta+2\theta+\alpha^*-1=0 
$$

$$\theta(-\alpha^*-\beta+2)+\alpha^*-1=0
$$

$$\theta(-\alpha^*-\beta+2) = 1-\alpha^*
$$

$$\theta^{-}_{MAP} = \frac{1-\alpha^*}{-\alpha^*-\beta^*+2}
$$

# Código de estimador bayesiano

## Creación de datos de prueba
Esto nos va a poder ayudar a comparar nuestro estimador bayesiano con el valor real de $\theta$ a partir de una distribución Binomial Negativa.

```{r}

rm(list = ls())
theta = .76
r = 10
n = 20
set.seed(1010)
x = rnbinom(n, size = r, prob = theta)

```


## A priori Beta no informativa de theta

```{r}
alpha = 1
beta = 1
p_range = seq(0, 1, .001)
priori_t = dbeta(p_range, alpha, beta)
plot(p_range, priori_t, col="magenta", type ="l", lwd = 2, xlab = "theta", ylab = "f(theta)", main = "Densidad a priori Beta para theta")
```

## A posteriori Beta de lambda

```{r}
alpha_p = (n*r) + alpha
beta_p = sum(x) + beta

post_t = dbeta(p_range, alpha_p, beta_p)

plot(p_range, post_t, col="dodgerblue", type ="l", lwd = 2, xlab = "theta", ylab = "f(theta)", main = "Densidad a Posteriori vs a Priori")
lines(p_range, priori_t, col="magenta", type ="l", lwd = 2)

IC_cred = qbeta(c(0.025, 0.975), alpha_p, beta_p)

MAP = (1 - alpha_p)/(-alpha_p-beta_p+2)
Mean = alpha_p/(alpha_p + beta_p)
Median = qbeta(0.5, alpha_p, beta_p)

abline(v = MAP, lwd = 2, col = "yellow", lty = 4)
abline(v = Mean, lwd = 2, col = "blue4", lty = 4)
abline(v = Median, lwd = 2, col = "green", lty = 4)
abline(v = IC_cred[1], lwd = 2, col = "red", lty = 5)
abline(v = IC_cred[2], lwd = 2, col = "red", lty = 6)
abline(v = theta, lwd = 2, col = "black", lty = 1)
legend("topright", c("MAP", "Mean", "Median", "IC_cred_lower", "IC_cred_upper", "Real value"), lty = c(4, 4, 4, 5, 6, 1), col = c("yellow", "blue4", "green", "red", "red", "black"))


cat("MAP = ", MAP, "\n")
cat("Mean = ", Mean, "\n")
cat("Median = ", Median, "\n")
cat("Intervalo de credibilidad = ", IC_cred, "\n")
cat("Valor Real = ", theta, "\n")

```

## Función de Verosimilitud

```{r}

verosim_v = c()
for (t in p_range) {
  verosim = t^{n*r} * (1-t)^{sum(x)}
  verosim_v = append(verosim_v, verosim)
}

MV = (n*r)/(sum(x)+(n*r))

threshold = MV * exp(-qchisq(0.95, df = 1) / 2)

IC_lower = min(p_range[verosim_v >= threshold])
IC_upper = max(p_range[verosim_v >= threshold])

plot(p_range, verosim_v, col="purple", type ="l", lwd = 2, xlab = "theta", ylab = "L(theta)", main = "Gráfica de verosimilitud")
abline(v = MV, lwd = 2, col = "gold", lty = 4)
abline(v = IC_lower , lwd = 2, col = "blue4", lty = 4)
abline(v = IC_upper, lwd = 2, col = "blue4", lty = 4)
abline(v = theta, lwd = 2, col = "black", lty = 1)
legend("topright", c("MV", "Int_conf", "Valor Real"), lty = c(4, 4, 1), col = c("gold", "blue4", "black"))


cat("MV = ", MV, "\n")
cat("Confidence Interval = ", IC_lower, ",", IC_upper)
```

---
title: "Estimador de theta en datos Geometricos con a priori Beta"
author: "Sofia Gamino Estrada"
date: "October 19, 2024"
output: html_document
---
---

En este código se puede dar una densidad de posibles valores para un parámetro theta desconocido de una distribución Geometrica en un intervalo de confianza a partir de n conocido por medio de inferencia Bayesiana

**Input**:
- n: Tamaño de la muestra
- theta: Para poder comparar el desempeño del modelo y definir los datos de prueba 
- alpha: Uno de los parámetros de la distribución beta
- beta: El otro parámetro de la distribución beta

**Output**: 
- Distribución a priori
- Distribución a posteriori
- Estimadores a posteriori: MAP, Media, Mediana
- Intervalo de credibilidad de lambda del 95%: Son los posibles valores de lambda
- Distribución de Verosimilitud, así como MV y intervalo de confianza(no sale): Para comparación con el método Bayesiano

---

# Descripción del problema
2. Suponga que se tiene una muestra aleatoria de tamaño $n$ de una variable aleatoria Geométrica$(\theta)$. Encuentre la distribución posterior de $\Theta$ considerando que, a priori, $\Theta$ sigue una distribución Beta$(a, b)$. Recuerde que la variable $X$ cuenta el número de fallas hasta el primer éxito, por lo que la función de probabilidad es:
$$ f_{X|\Theta}(x|\theta) = \begin{cases}
\theta(1-\theta)^x, & x=0,1,2,...;\\
0 & \text{otro caso}.
\end{cases}
$$

Distribución beta a priori
$$f_{\Theta}(\theta) = \begin{cases}
\frac{ 1}{B(\alpha,\beta)} x^{\alpha-1} (1-x)^{\beta-1}, & x>0;\\
0, & \text{otro caso}.
\end{cases}
$$

# Distribución Conjugada

## Hallar los kernel

Primero tenemos que hallar el kernel de $f_{X|\Theta} (x|\theta)$ y de $f_{\theta}(\theta)$

### Kernel de $f_{X|\Theta} (x|\theta)$

$$f_{X|\Theta} (x|\theta) = 
\theta(1-\theta)^x
$$

$$L(\theta) = \prod^n_{i=1}(f_{X|\Theta} (x|\theta)) = \prod^n_{i=1}\theta(1-\theta)^x
$$

$$L(\theta) = \theta^n(1-\theta)^{s}, s = \sum^n_{i=1}x_i
$$

### Kernel de $f_{\Lambda}(\lambda)$

$$f_{\Theta}(\theta) = 
\frac{ 1}{B(\alpha,\beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1}
$$

$$f_{\Theta}(\theta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}
$$

## Distribución Conjugada a posteriori
Ya que tenemos los kernel, podemos sacar la distribución conjugada a posteriori. En este caso, ignoraremos la constante de normalización $\int¹_{0}L(\theta)f_{\Theta}(\theta)d\theta$ en el denominador.

$$(\theta|x_1, ..., x_n) = 
L(\theta)f_{\Theta}(\theta)
$$

$$(\theta|x_1, ..., x_n) = 
\theta^n(1-\theta)^{s}\theta^{\alpha-1} (1-\theta)^{\beta-1}
$$

$$(\theta|x_1, ..., x_n) = 
\theta^{n+\alpha-1}(1-\theta)^{s+\beta-1}\ 
$$

$$(\theta|x_1, ..., x_n) = 
\theta^{a^*-1}(1-\theta)^{\beta^*-1}, \space \alpha^*=n+\alpha, \space \beta^* = s+\beta
$$

## Estimación de MAP

Podemos hallar MAP de theta a partir de encontrar cuando la distribución conjugada es 0. Se hace ln(L($\theta$)), después se deriva $\theta$ y finalmente igualamos a 0.

$$l(\theta) = 
ln[\theta^{\alpha^*-1}(1-\theta)^{\beta^*-1}]
$$
$$l(\theta)=
ln(\theta^{\alpha^*-1})+ln((1-\theta)^{\beta^*-1})
$$

$$l(\theta) = 
(\alpha^*-1)ln(\theta)-(\beta^*-1)ln(1-\theta)
$$

$$\frac{\partial l(\theta)}{\partial \theta} = \frac{\partial}{\partial\theta}[(\alpha^*-1)ln(\theta)-(\beta^*-1)ln(1-\theta)]
$$

$$ = 
\frac{\alpha^*-1}{\theta}-\frac{\beta^*-1}{1-\theta}
$$

$$ = 
\frac{\alpha^*-1(1-\theta)-\theta(\beta^*-1)}{\theta(1-\theta)}
$$
$$\frac{\alpha^*-\alpha^*\theta-1+\theta-\beta^*\theta+\theta}{\theta(1-\theta)}=0
$$
$$-\alpha^*\theta-\beta^*\theta+2\theta+\alpha^*-1=0 
$$

$$\theta(-\alpha^*-\beta+2)+\alpha^*-1=0
$$

$$\theta(-\alpha^*-\beta+2) = 1-\alpha^*
$$

$$\theta^{-}_{MAP} = \frac{1-\alpha^*}{-\alpha^*-\beta^*+2}
$$

# Código de estimador bayesiano

## Creación de datos de prueba
Esto nos va a poder ayudar a comparar nuestro estimador bayesiano con el valor real de $\theta$ a partir de una distribución Geometrica.

```{r}
rm(list = ls())
theta = .76
n = 20
set.seed(1010)
x = rgeom(n, theta)

```


## A priori Beta no informativa de theta

```{r}
alpha = 1
beta = 1
p_range = seq(0, 1, .001)
priori_t = dbeta(p_range, alpha, beta)
plot(p_range, priori_t, col="magenta", type ="l", lwd = 2, xlab = "theta", ylab = "f(theta)", main = "Densidad a priori Beta para theta")
```

## A posteriori Beta de lambda

```{r}
alpha_p = n + alpha
beta_p = sum(x) + beta

post_t = dbeta(p_range, alpha_p, beta_p)

plot(p_range, post_t, col="dodgerblue", type ="l", lwd = 2, xlab = "theta", ylab = "f(theta)", main = "Densidad a Posteriori vs a Priori")
lines(p_range, priori_t, col="magenta", type ="l", lwd = 2)

IC_cred = qbeta(c(0.025, 0.975), alpha_p, beta_p)

MAP = (1 - alpha_p)/(-alpha_p-beta_p+2)
Mean = alpha_p/(alpha_p + beta_p)
Median = qbeta(0.5, alpha_p, beta_p)

abline(v = MAP, lwd = 2, col = "yellow", lty = 4)
abline(v = Mean, lwd = 2, col = "blue4", lty = 4)
abline(v = Median, lwd = 2, col = "green", lty = 4)
abline(v = IC_cred[1], lwd = 2, col = "red", lty = 5)
abline(v = IC_cred[2], lwd = 2, col = "red", lty = 6)
abline(v = theta, lwd = 2, col = "black", lty = 1)
legend("topright", c("MAP", "Mean", "Median", "IC_cred_lower", "IC_cred_upper", "Real value"), lty = c(4, 4, 4, 5, 6, 1), col = c("yellow", "blue4", "green", "red", "red", "black"))


cat("MAP = ", MAP, "\n")
cat("Mean = ", Mean, "\n")
cat("Median = ", Median, "\n")
cat("Intervalo de credibilidad = ", IC_cred, "\n")
cat("Valor Real = ", theta, "\n")

```

## Función de Verosimilitud

```{r}

verosim_v = c()
for (t in p_range) {
  verosim = t^{n} * (1-t)^{sum(x)}
  verosim_v = append(verosim_v, verosim)
}

MV = n/(n + sum(x))

threshold = MV * exp(-qchisq(0.95, df = 1) / 2)

IC_lower = min(p_range[verosim_v >= threshold])
IC_upper = max(p_range[verosim_v >= threshold])

plot(p_range, verosim_v, col="purple", type ="l", lwd = 2, xlab = "theta", ylab = "L(theta)", main = "Gráfica de verosimilitud")
abline(v = MV, lwd = 2, col = "gold", lty = 4)
abline(v = IC_lower , lwd = 2, col = "blue4", lty = 4)
abline(v = IC_upper, lwd = 2, col = "blue4", lty = 4)
abline(v = theta, lwd = 2, col = "black", lty = 1)
legend("topright", c("MV", "Int_conf", "Valor Real"), lty = c(4, 4, 1), col = c("gold", "blue4", "black"))


cat("MV = ", MV, "\n")
cat("Confidence Interval = ", IC_lower, ",", IC_upper)
```

